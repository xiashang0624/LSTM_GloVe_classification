{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys, os, csv\n",
    "from collections import defaultdict\n",
    "from scipy.sparse import csr_matrix, csc_matrix, diags\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import itertools\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout, Input, LSTM, Embedding, Activation, Bidirectional, GlobalMaxPool1D\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Accepted_answer_prediction_data_train.txt', sep ='\\t',\n",
    "                 header=None,names=['C_ID','Description','U_ID','Type','Time'])\n",
    "label = pd.read_csv('Accepted_answer_prediction_labels_train.txt', sep ='\\t',\n",
    "                 header=None,names=['C_ID','label'])\n",
    "df = pd.merge(df, label, on=['C_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_file='glove.6B.100d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['length'] = [len(str(des).split()) for des in df['Description'].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_D = 100 # Dimension of the embedding.  Here we chose 100\n",
    "doc_len = 300 # we chose certain number of words used in the model. For documents with fewer words, zero padding will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "Prepare X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_Doc = df[\"Description\"].fillna(\"NA\").values\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(list(Train_Doc))\n",
    "Train_tokenize = tokenizer.texts_to_sequences(Train_Doc)\n",
    "X_t = pad_sequences(Train_tokenize, maxlen=doc_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = df['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding\n",
    "Since the given dataset has been preprocess. we will apply stemming on the embedding file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding = {}\n",
    "f = open(embedding_file, encoding=\"utf8\")\n",
    "for word_vec in f:\n",
    "    word_vec = word_vec.split()\n",
    "    word = word_vec[0]\n",
    "    vec = np.asarray(word_vec[1:])\n",
    "    embedding[p.stem(word)] = vec.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('the length of the embedding volcabulary after stemming treatment is ', len(embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiate the embedding matrix with random numbers based on the mean and std of the embedding file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "emb_values = np.stack(embedding.values())\n",
    "emb_mean = emb_values.mean()\n",
    "emb_std = emb_values.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = tokenizer.word_index\n",
    "vocab_len = len(vocab)+1 # here the reason of adding 1 is because the index of word in vocab starts at 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed_Matrix = np.random.normal(emb_mean, emb_std, (vocab_len, emb_D))\n",
    "for w, i in vocab.items():\n",
    "    embed_Vector = embedding.get(w)\n",
    "    if embed_Vector is not None: \n",
    "        embed_Matrix[i] = embed_Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the tokenizer for testing\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def sensitivity(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    return true_positives / (possible_positives + K.epsilon())\n",
    "\n",
    "def specificity(y_true, y_pred):\n",
    "    true_negatives = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))\n",
    "    possible_negatives = K.sum(K.round(K.clip(1-y_true, 0, 1)))\n",
    "    return true_negatives / (possible_negatives + K.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = Input(shape=(doc_len,))\n",
    "b = Embedding(vocab_len, emb_D, weights=[embed_Matrix])(a)\n",
    "b = Bidirectional(LSTM(100, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))(b)\n",
    "b = GlobalMaxPool1D()(b)\n",
    "b = Dense(100, activation=\"elu\")(b)\n",
    "b = Dropout(0.2)(b)\n",
    "b = Dense(50, activation=\"elu\")(b)\n",
    "b = Dropout(0.2)(b)\n",
    "# b = Dense(20, activation=\"elu\")(b)\n",
    "# b = Dropout(0.15)(b)\n",
    "b = Dense(1, activation=\"sigmoid\")(b)\n",
    "model = Model(inputs=a, outputs=b)\n",
    "model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=[sensitivity, specificity, 'accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-fold cross-validation\n",
    "folds = 4\n",
    "kfold = StratifiedKFold(n_splits=folds, shuffle=True)\n",
    "iter_kfold = 0\n",
    "validation_result = []\n",
    "total_result = []\n",
    "for train, test in kfold.split(X_t, y):\n",
    "    class_weights = class_weight.compute_class_weight('balanced',np.unique(y[train]),y[train])\n",
    "    model.fit(X_t[train], y[train], batch_size=32, epochs=10, validation_split=0.2,class_weight={0:class_weights[0],1:class_weights[1]});\n",
    "    validation_result.append(model.evaluate(X_t[test], y[test], verbose=0))\n",
    "    total_result.append(model.evaluate(X_t, y, verbose=0))\n",
    "    model_name = 'model_' + str(iter_kfold) + '.h5'\n",
    "    model.save(model_name)\n",
    "    iter_kfold += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f=open('training_result.txt', 'w')\n",
    "now = datetime.now()\n",
    "f.write('Model trained at Timestamp: {:%Y-%m-%d %H:%M:%S}\\n'.format(datetime.now()))\n",
    "f.write('{}-fold validation\\n'.format(folds))\n",
    "for i in range(folds):\n",
    "    f.write('model {0:.0f}: Recall={1:.3f} Selectivity={2:.3f} Accuracy={3:.3f}.\\n'.format(i, validation_result[i][1], validation_result[i][2],validation_result[i][3]))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import metrics\n",
    "# prediction = model.predict(X_t[test])\n",
    "# metrics.roc_auc_score(y[test], prediction)\n",
    "# output = np.round(prediction)\n",
    "# print ('conf_matrix',metrics.confusion_matrix(y[test],output))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
